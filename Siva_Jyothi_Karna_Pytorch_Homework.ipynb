{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97913248d3b14d29b10bd8220325b9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfebcf125d80407996419345d1e38821",
              "IPY_MODEL_0928af7f37594add8a04730faf0fddf2",
              "IPY_MODEL_b336363bc36f451e9731bfc2befd7b3e"
            ],
            "layout": "IPY_MODEL_4c31d2f799974c629f490eefbe33aac6"
          }
        },
        "cfebcf125d80407996419345d1e38821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13055ec7d9ad4be79a30edaa50cb7f08",
            "placeholder": "​",
            "style": "IPY_MODEL_2e5f76add7844e299c926c1a817a6654",
            "value": "100%"
          }
        },
        "0928af7f37594add8a04730faf0fddf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc3d1353159543149d5efcd467eb1f5c",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_245793c77e34494b9e516f019d010be7",
            "value": 170498071
          }
        },
        "b336363bc36f451e9731bfc2befd7b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f176c4693254a8ea539796407c67a94",
            "placeholder": "​",
            "style": "IPY_MODEL_abfa82ba68854fe686fb39c696e43e5b",
            "value": " 170498071/170498071 [00:14&lt;00:00, 13939254.04it/s]"
          }
        },
        "4c31d2f799974c629f490eefbe33aac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13055ec7d9ad4be79a30edaa50cb7f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e5f76add7844e299c926c1a817a6654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc3d1353159543149d5efcd467eb1f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "245793c77e34494b9e516f019d010be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f176c4693254a8ea539796407c67a94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abfa82ba68854fe686fb39c696e43e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JyothiKarna/High-Performance-Computing/blob/main/Siva_Jyothi_Karna_Pytorch_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_LTBtfMBvW_"
      },
      "source": [
        "# Pytorch homework\n",
        "\n",
        "### Instructions\n",
        "- Make a copy of this notebook in your own Colab and complete the questions there.\n",
        "- You can add more cells if necessary. You may also add descriptions to your code, though it is not mandatory.\n",
        "- Make sure the notebook can run through by *Runtime -> Run all*. **Keep all cell outputs** for grading.\n",
        "- Submit the link of your notebook \n",
        "- Please **enable editing or comments** so that you can receive feedback from TAs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T33dD1e8tii2"
      },
      "source": [
        "Install PyTorch and Skorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJB3VQYDCUmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91a1b1c-d2c6-4701-a9f1-f645c92d887f"
      },
      "source": [
        "!pip install -q torch skorch torchvision torchtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 30 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 81 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 92 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 112 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 122 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 143 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 153 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 163 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 174 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 184 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 193 kB 14.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l_Dl6qxCXmv"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import skorch\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uevQtU7NtZ_-"
      },
      "source": [
        "## 1. Tensor Operations (40 points)\n",
        "\n",
        "Tensor operations are important in deep learning models. In this part, you are required to implement some common tensor operations in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DeQOItkeQCx"
      },
      "source": [
        "### 1) Tensor squeezing, unsqueezing and viewing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAOmBE5ODwpP"
      },
      "source": [
        "Tensor squeezing, unsqueezing and viewing are important methods to change the dimension of a Tensor, and the corresponding functions are [torch.squeeze](https://pytorch.org/docs/stable/torch.html#torch.squeeze), [torch.unsqueeze](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze) and [torch.Tensor.view](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view). Please read the documents of the functions, and finish the following practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVrM80YxFSjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b339623-da72-4afa-e2e8-6b242fd7adcb"
      },
      "source": [
        "# x is a tensor with size being (3, 2)\n",
        "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add two new dimensions to x by using the function torch.unsqueeze, so that the size of x becomes (3, 1, 2, 1).\n",
        "x = torch.unsqueeze(x,1)\n",
        "x = torch.unsqueeze(x,-1)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN6bDvSgezbt",
        "outputId": "22dbb595-38a5-4b1a-d03a-d1c2b32f4a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the two dimensions justed added by using the function torch.squeeze, and change the size of x back to (3, 2).\n",
        "x = torch.squeeze(x,1)\n",
        "x = torch.squeeze(x,2)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4kZzj5Pe2nx",
        "outputId": "7f18f598-65b3-4e97-d98b-6a5f57927307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x is now a two-dimensional tensor, or in other words a matrix. Now use the function torch.Tensor.view and change x to a one-dimensional vector with size being (6).\n",
        "x = x.view(6)\n",
        "print(\"One Dimensional Vector: \", x)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBM4yLIwe21g",
        "outputId": "411ddf33-6f99-48c0-aba7-dcece43f0f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One Dimensional Vector:  tensor([1., 2., 3., 4., 5., 6.])\n",
            "torch.Size([6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liuR-U0wea0n"
      },
      "source": [
        "### 2) Tensor concatenation and stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkbnt6v8Bo-j"
      },
      "source": [
        "Tensor concatenation and stack are operations to combine small tensors into big tensors. The corresponding functions are [torch.cat](https://pytorch.org/docs/stable/torch.html#torch.cat) and [torch.stack](https://pytorch.org/docs/stable/torch.html#torch.stack). Please read the documents of the functions, and finish the following practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9KqXu3Stfjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f1dc19-3474-443e-dbdd-7486ef48c0a2"
      },
      "source": [
        "# x is a tensor with size being (3, 2)\n",
        "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
        "print(\"Size of x Tensor: \", x.shape)\n",
        "\n",
        "# y is a tensor with size being (3, 2)\n",
        "y = torch.Tensor([[-1, -2], [-3, -4], [-5, -6]])\n",
        "print(\"Size of y Tensor: \", y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of x Tensor:  torch.Size([3, 2])\n",
            "Size of y Tensor:  torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our goal is to generate a tensor z with size as (2, 3, 2), and z[0,:,:] = x, z[1,:,:] = y.\n",
        "# Use torch.stack to generate such a z\n",
        "z = torch.stack((x,y),0)\n",
        "print(\"Stack z: \",z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McBC7GMppsqI",
        "outputId": "de2acbcd-6797-4a92-d5ea-3ab27bb4c5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stack z:  tensor([[[ 1.,  2.],\n",
            "         [ 3.,  4.],\n",
            "         [ 5.,  6.]],\n",
            "\n",
            "        [[-1., -2.],\n",
            "         [-3., -4.],\n",
            "         [-5., -6.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use torch.cat and torch.unsqueeze to generate such a z\n",
        "z = torch.cat((x,y),0)\n",
        "print(\"Concatenate z: \",z)\n",
        "print(z.shape)\n",
        "\n",
        "# To unsqueeze data:\n",
        "z = torch.unsqueeze(z,1)\n",
        "print(z.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK-BLDPcpu0x",
        "outputId": "75b3df15-3e17-44f3-93a1-b5480bb55aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenate z:  tensor([[ 1.,  2.],\n",
            "        [ 3.,  4.],\n",
            "        [ 5.,  6.],\n",
            "        [-1., -2.],\n",
            "        [-3., -4.],\n",
            "        [-5., -6.]])\n",
            "torch.Size([6, 2])\n",
            "torch.Size([6, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGw4eEo-eeHm"
      },
      "source": [
        "### 3) Tensor expansion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAII9eJgJFK2"
      },
      "source": [
        "Tensor expansion is to expand a tensor into a larger tensor along singleton dimensions. The corresponding functions are [torch.Tensor.expand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand) and [torch.Tensor.expand_as](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand_as). Please read the documents of the functions, and finish the following practice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQbFte-AJzVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef6b555-56c4-4723-8696-28ce90d506aa"
      },
      "source": [
        "# x is a tensor with size being (3)\n",
        "x = torch.Tensor([1, 2, 3])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our goal is to generate a tensor z with size (2, 3), so that z[0,:,:] = x, z[1,:,:] = x.\n",
        "\n",
        "# [TO DO]\n",
        "# Change the size of x into (1, 3) by using torch.unsqueeze.\n",
        "x = torch.unsqueeze(x,0)\n",
        "print(\"Amended unsqueeze tensor: \",x)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1Yz0P1-xNRi",
        "outputId": "f6cbc9c4-2d78-4909-d65a-cdfd53933ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amended unsqueeze tensor:  tensor([[1., 2., 3.]])\n",
            "torch.Size([1, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [TO DO]\n",
        "# Then expand the new tensor to the target tensor by using torch.Tensor.expand.\n",
        "x_new = x.expand(3,-1)\n",
        "print(\"Expand the new tensor: \", x_new)\n",
        "x_new.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njzWAcfpxNzn",
        "outputId": "f47c0785-93eb-41b5-d4f7-109730ecf04b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expand the new tensor:  tensor([[1., 2., 3.],\n",
            "        [1., 2., 3.],\n",
            "        [1., 2., 3.]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rFL_Shoef3m"
      },
      "source": [
        "### 4) Tensor reduction in a given dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmEoJVw0LL9H"
      },
      "source": [
        "In deep learning, we often need to compute the mean/sum/max/min value in a given dimension of a tensor. Please read the document of [torch.mean](https://pytorch.org/docs/stable/torch.html#torch.mean), [torch.sum](https://pytorch.org/docs/stable/torch.html#torch.sum), [torch.max](https://pytorch.org/docs/stable/torch.html#torch.max), [torch.min](https://pytorch.org/docs/stable/torch.html#torch.min), [torch.topk](https://pytorch.org/docs/stable/torch.html#torch.topk), and finish the following practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7dlZwe4MNxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46605135-b09c-4901-ec20-ae3da4c3e378"
      },
      "source": [
        "# x is a random tensor with size being (10, 50)\n",
        "x = torch.randn(10,50)\n",
        "print(\"Random tensor with size (10,50): \", x)\n",
        "print(x.shape)\n",
        "\n",
        "# Compute the mean value for each row of x.\n",
        "# You need to generate a tensor x_mean of size (10), and x_mean[k, :] is the mean value of the k-th row of x.\n",
        "\n",
        "mean = torch.mean(x, axis=1)\n",
        "print(\"Mean value for each row of x: \", mean)\n",
        "print(mean.shape)\n",
        "\n",
        "print(\"Mean value of the k-th row of x: \", mean.data[5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random tensor with size (10,50):  tensor([[-8.5858e-02,  2.5602e+00,  4.1812e-02,  8.0594e-01, -1.5743e-01,\n",
            "          7.7663e-01,  3.7708e-01, -6.2573e-01, -4.4539e-01, -1.2694e+00,\n",
            "          1.3030e+00,  1.1802e-01, -4.9406e-01, -1.1608e+00,  3.5191e+00,\n",
            "         -1.1971e+00, -1.8871e-01,  1.1577e+00,  3.8007e-01, -3.1119e-01,\n",
            "         -7.7780e-01, -7.0767e-01, -5.7530e-02, -6.4098e-01, -1.0862e+00,\n",
            "          7.0629e-02,  9.0801e-01, -1.1436e+00, -1.0673e+00,  1.2744e+00,\n",
            "          1.6051e+00,  4.5315e-02,  6.3402e-01, -2.0864e-01,  1.2202e+00,\n",
            "          1.7874e+00,  2.0678e+00,  1.0857e+00,  1.8517e-01,  6.2265e-01,\n",
            "         -2.9751e-01, -2.2185e-01,  1.6758e-01,  2.2956e+00,  8.0153e-01,\n",
            "         -2.0041e-01, -4.6298e-01,  1.1350e-01,  7.6735e-01, -1.4846e-01],\n",
            "        [-1.9866e-02, -9.6344e-01,  1.2482e+00, -4.0255e-01, -1.1671e+00,\n",
            "          8.9451e-01, -1.2777e+00, -1.2267e-02, -6.9263e-01,  6.8580e-01,\n",
            "         -5.5002e-01, -4.5871e-01, -1.1414e+00, -9.6169e-01,  8.1608e-01,\n",
            "          1.3305e+00,  3.8405e-01,  2.8065e-01,  9.3392e-01, -6.3131e-01,\n",
            "         -1.9645e+00, -1.3457e+00,  1.4257e+00,  1.4868e+00,  9.4781e-01,\n",
            "          6.5627e-01,  2.1610e-01, -1.7670e+00, -7.7685e-01,  3.2851e-01,\n",
            "          1.4335e+00,  3.9298e-01,  8.8987e-01,  4.1576e-01, -1.9504e-01,\n",
            "         -2.4178e+00,  2.3489e-01, -1.9262e-01,  6.7372e-01,  1.7975e+00,\n",
            "         -3.5983e-01,  1.3039e+00, -2.1503e-01,  1.5218e+00, -1.4518e+00,\n",
            "          1.5007e+00,  6.2903e-01, -8.1042e-01, -7.3514e-01,  2.0869e-01],\n",
            "        [-5.9743e-01,  8.5062e-01,  6.6777e-01, -1.3106e+00, -9.0545e-01,\n",
            "         -9.2912e-01, -6.8175e-01, -5.2682e-01,  2.6521e-01, -1.6631e+00,\n",
            "          2.1095e+00,  2.0984e-01, -1.0481e+00, -9.2857e-01,  3.4069e-01,\n",
            "          5.2897e-01,  2.6118e-02,  6.4751e-02,  2.9889e-01, -1.2182e+00,\n",
            "         -6.3572e-01,  1.8630e+00, -3.8774e-01, -6.7631e-01, -1.1800e+00,\n",
            "          5.7473e-01,  8.1448e-02, -3.8863e-01,  9.7720e-01, -6.3296e-01,\n",
            "         -6.8800e-01,  4.0751e-01,  7.8442e-01,  2.9278e-01,  2.0308e+00,\n",
            "         -6.2870e-01,  1.5194e-01,  3.7035e-01,  2.1906e-01,  1.0927e+00,\n",
            "         -7.5670e-02, -5.9124e-01,  2.6415e-01,  4.3046e-01,  1.0933e+00,\n",
            "          1.7074e+00,  2.2867e+00, -1.0717e+00,  8.1794e-02, -1.1265e+00],\n",
            "        [ 2.5490e-01, -5.2822e-02, -5.1440e-01, -2.6734e-01,  6.4735e-01,\n",
            "         -6.5943e-01, -5.2733e-01,  8.1851e-01,  1.3899e-01, -7.4601e-01,\n",
            "         -1.0391e-03,  5.6500e-01, -1.3827e+00,  1.4119e+00, -1.9081e-01,\n",
            "          3.2168e+00, -6.9448e-01, -1.3790e+00, -4.6277e-01, -4.4186e-01,\n",
            "          7.5983e-01, -5.9374e-01, -4.1134e-01,  1.6332e+00, -8.7987e-01,\n",
            "         -6.9588e-02,  1.5149e+00, -8.0459e-01, -3.5899e-01,  1.5133e-01,\n",
            "          4.0134e-01,  2.8131e-01,  3.1294e-01, -6.1371e-01,  8.4653e-01,\n",
            "         -1.0237e-01, -8.0901e-01,  4.8914e-02,  3.1021e-01, -1.9441e+00,\n",
            "         -1.0399e+00,  2.7172e-01,  8.3928e-01,  1.6088e+00, -2.8387e-01,\n",
            "          2.3876e+00, -1.1741e+00, -9.5626e-02,  2.3786e-02,  7.4120e-02],\n",
            "        [-2.4830e-01,  1.1554e+00, -1.7492e+00,  1.0733e+00,  4.2043e-01,\n",
            "          7.6204e-01, -1.9749e-01, -8.9862e-02, -1.3473e-01,  1.7338e+00,\n",
            "         -7.6425e-01,  4.9154e-01, -8.3662e-01,  2.0461e-01, -3.2649e-02,\n",
            "          9.8736e-01, -4.1799e-01, -1.1155e+00,  4.1934e-02,  6.0702e-01,\n",
            "          9.8702e-02, -3.4316e-01, -1.3388e+00,  5.4891e-02, -2.4637e-01,\n",
            "          7.9205e-02, -1.1495e+00,  9.0386e-01, -5.2408e-01,  7.3823e-01,\n",
            "         -1.2306e+00,  7.7722e-01, -2.5195e-01, -1.6626e+00,  1.2888e+00,\n",
            "          1.2792e+00,  5.0684e-01,  1.4202e+00, -2.6644e+00,  1.2274e+00,\n",
            "          1.1022e+00, -1.1252e-01, -6.5440e-01, -1.6187e-01,  5.5576e-01,\n",
            "         -3.7423e-01,  5.1194e-01, -5.0410e-01, -8.8810e-01, -1.5051e+00],\n",
            "        [ 9.6479e-01, -1.1509e+00, -4.1306e-02, -1.2131e+00,  1.8683e+00,\n",
            "          4.7766e-01, -5.4528e-01, -3.0634e-01,  1.0049e+00,  3.2522e-01,\n",
            "         -6.0616e-01, -1.0318e+00,  2.2307e-01, -1.3873e+00,  1.0034e+00,\n",
            "         -9.5115e-01,  1.5305e+00,  5.2435e-01, -7.8125e-01, -2.4530e-01,\n",
            "         -1.4751e+00,  3.3449e-01,  1.0223e+00, -6.3168e-01,  1.8302e-01,\n",
            "         -7.3994e-01, -3.2792e-01, -1.1245e-01, -1.3973e+00, -1.8620e+00,\n",
            "          5.0378e-01, -7.2766e-01,  1.1330e+00, -8.8114e-01, -7.6077e-01,\n",
            "         -1.1180e-01,  1.2107e+00, -1.9707e+00,  1.5445e-01, -2.5212e-01,\n",
            "          6.8628e-01,  4.9245e-01, -3.2731e-01,  2.0632e+00, -2.4904e-01,\n",
            "          3.5390e-01,  1.9204e-01,  9.3988e-02,  8.8003e-01,  2.9641e-01],\n",
            "        [-2.4231e+00, -4.5318e-01,  1.8521e+00,  5.8949e-01, -1.1718e+00,\n",
            "         -5.6959e-02,  3.3425e-01, -2.1834e+00, -1.7086e+00, -8.0554e-01,\n",
            "          9.1432e-01,  1.3309e+00, -4.0050e-02, -1.0281e+00,  6.8475e-01,\n",
            "          1.3488e+00,  2.0031e-01, -2.9855e-01, -9.3500e-01,  5.5612e-01,\n",
            "         -6.7657e-01,  2.3416e-01,  2.0836e-01,  1.0945e-01,  4.5260e-01,\n",
            "         -1.1303e-01,  1.2574e+00, -1.3791e+00, -3.9248e-01,  1.3289e-01,\n",
            "         -9.8147e-01, -8.2860e-01, -1.3596e-01,  6.2857e-01, -3.9767e-01,\n",
            "         -7.6730e-01,  1.5380e+00, -1.2112e+00,  2.3144e-01,  1.6413e+00,\n",
            "         -1.4657e+00,  3.6826e-01,  2.0072e+00,  5.0769e-01,  2.3410e-01,\n",
            "          2.7169e-01,  3.0638e-01, -2.4266e-01, -1.1408e+00, -2.5513e-01],\n",
            "        [ 2.3966e-01, -1.6798e+00,  1.8155e+00,  8.4843e-01, -1.6070e+00,\n",
            "          7.2588e-01,  5.6545e-01,  1.1847e+00,  1.5568e+00,  7.2472e-01,\n",
            "          1.4925e+00,  2.7946e-01,  5.4655e-01, -3.0654e-01,  7.3321e-02,\n",
            "         -1.1662e+00,  1.9675e+00, -7.3164e-02, -8.6481e-01, -8.4063e-01,\n",
            "         -1.1715e+00, -5.0288e-01, -3.6026e-01,  6.2497e-01,  6.0948e-01,\n",
            "         -1.6449e+00,  8.5463e-01, -1.0495e+00, -9.6230e-01, -1.1080e+00,\n",
            "          4.2396e-01, -6.8602e-01, -1.9711e+00,  1.4418e+00, -1.5050e-01,\n",
            "         -1.0761e+00, -1.2914e+00, -5.6402e-01, -1.0876e+00, -1.5317e+00,\n",
            "          8.6916e-02, -1.1391e+00, -1.4264e-01, -9.7700e-01,  2.1425e-01,\n",
            "         -1.7069e+00,  1.2014e+00, -7.9426e-02, -1.2833e+00,  2.3248e-01],\n",
            "        [ 1.3001e+00,  9.3495e-01, -1.6085e+00,  4.5107e-01, -3.7247e-01,\n",
            "          6.2967e-01, -3.5706e-01,  2.7632e-02, -4.4757e-01,  7.7520e-01,\n",
            "          6.8475e-01, -1.1240e+00, -6.7373e-01,  8.3308e-01,  1.8101e+00,\n",
            "         -2.6850e-01, -9.6688e-02,  4.1641e-01, -2.3115e-01, -2.7877e-01,\n",
            "         -7.8312e-01, -8.2952e-01,  5.8823e-01, -4.0623e-01, -6.6699e-01,\n",
            "         -1.1729e+00, -2.0003e+00,  1.6857e+00,  1.3409e+00, -1.0544e+00,\n",
            "         -1.1641e+00,  4.0932e-01,  1.0890e+00, -3.3304e-01,  3.3158e-01,\n",
            "         -7.2455e-01, -1.0399e+00,  4.8344e-01, -6.1528e-01, -1.0616e+00,\n",
            "         -4.7894e-01, -2.9970e+00, -2.7801e-01,  1.6479e+00, -2.1104e+00,\n",
            "         -2.5534e-02, -2.0180e-01, -1.4037e-01,  1.1659e+00, -1.6481e-01],\n",
            "        [ 7.2965e-02,  2.9570e-01,  1.7944e+00, -1.4259e+00,  3.5088e-01,\n",
            "          8.6420e-01, -1.3846e+00,  1.6248e+00,  1.0271e+00,  6.4964e-01,\n",
            "          1.2214e+00,  4.5621e-01, -2.6136e-01,  6.7403e-01, -4.4160e-01,\n",
            "         -1.5888e+00, -8.0444e-01, -8.1041e-01,  3.7793e-01,  6.1797e-01,\n",
            "         -5.7132e-01,  2.2997e+00,  1.5958e+00,  7.2214e-01,  3.6376e-01,\n",
            "          1.9064e-01, -2.9010e-01,  9.6946e-01,  1.0590e+00, -2.9840e-02,\n",
            "         -8.0100e-02, -1.1985e+00, -1.1035e+00, -4.7235e-01,  2.0156e+00,\n",
            "         -7.4338e-01, -3.9749e-01,  1.1694e+00, -1.6562e+00, -4.7281e-01,\n",
            "         -7.2511e-02,  1.2822e+00, -9.8437e-01,  1.3215e-01,  1.5288e+00,\n",
            "          1.6518e-01,  5.8435e-02, -1.9326e-02,  9.5620e-01, -3.2288e-01]])\n",
            "torch.Size([10, 50])\n",
            "Mean value for each row of x:  tensor([ 0.2747,  0.0425,  0.0436,  0.0404, -0.0235, -0.0513, -0.0630, -0.1863,\n",
            "        -0.1420,  0.1881])\n",
            "torch.Size([10])\n",
            "Mean value of the k-th row of x:  tensor(-0.0513)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the sum value for each row of x.\n",
        "# You need to generate a tensor x_sum of size (10).\n",
        "sum = torch.sum(x,1)\n",
        "print(\"Sum value of each row: \", sum)"
      ],
      "metadata": {
        "id": "B45mXIObxjGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9035a4-8286-49cc-87cc-f6dcf7040fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum value of each row:  tensor([13.7348,  2.1271,  2.1797,  2.0186, -1.1766, -2.5645, -3.1515, -9.3138,\n",
            "        -7.1022,  9.4039])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the max value for each row of x.\n",
        "# You need to generate a tensor x_max of size (10).\n",
        "max = torch.max(x,1, out=None)\n",
        "print(\"Max value of each row: \", max)"
      ],
      "metadata": {
        "id": "MZsIJeHbxjO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6b25b3-db97-4545-a254-549330a5617e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max value of each row:  torch.return_types.max(\n",
            "values=tensor([3.5191, 1.7975, 2.2867, 3.2168, 1.7338, 2.0632, 2.0072, 1.9675, 1.8101,\n",
            "        2.2997]),\n",
            "indices=tensor([14, 39, 46, 15,  9, 43, 42, 16, 14, 21]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the min value for each row of x.\n",
        "# You need to generate a tensor x_min of size (10).\n",
        "min = torch.min(x,1)\n",
        "print(\"Min value of each row: \", min)"
      ],
      "metadata": {
        "id": "sf-DovjwxjWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd175f3d-0d33-40ec-a569-dda35f2f23c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min value of each row:  torch.return_types.min(\n",
            "values=tensor([-1.2694, -2.4178, -1.6631, -1.9441, -2.6644, -1.9707, -2.4231, -1.9711,\n",
            "        -2.9970, -1.6562]),\n",
            "indices=tensor([ 9, 35,  9, 39, 38, 37,  0, 32, 41, 38]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the top-5 values for each row of x.\n",
        "# You need to generate a tensor x_mean of size (10, 5), and x_top[k, :] is the top-5 values of each row in x.\n",
        "top_5 = torch.topk(x, 5)\n",
        "print(\"Top 5 values of each row: \", top_5)"
      ],
      "metadata": {
        "id": "MkfHUHyHxjd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51666934-4d7f-42fe-f09e-db3bd5b53e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 values of each row:  torch.return_types.topk(\n",
            "values=tensor([[3.5191, 2.5602, 2.2956, 2.0678, 1.7874],\n",
            "        [1.7975, 1.5218, 1.5007, 1.4868, 1.4335],\n",
            "        [2.2867, 2.1095, 2.0308, 1.8630, 1.7074],\n",
            "        [3.2168, 2.3876, 1.6332, 1.6088, 1.5149],\n",
            "        [1.7338, 1.4202, 1.2888, 1.2792, 1.2274],\n",
            "        [2.0632, 1.8683, 1.5305, 1.2107, 1.1330],\n",
            "        [2.0072, 1.8521, 1.6413, 1.5380, 1.3488],\n",
            "        [1.9675, 1.8155, 1.5568, 1.4925, 1.4418],\n",
            "        [1.8101, 1.6857, 1.6479, 1.3409, 1.3001],\n",
            "        [2.2997, 2.0156, 1.7944, 1.6248, 1.5958]]),\n",
            "indices=tensor([[14,  1, 43, 36, 35],\n",
            "        [39, 43, 45, 23, 30],\n",
            "        [46, 10, 34, 21, 45],\n",
            "        [15, 45, 23, 43, 26],\n",
            "        [ 9, 37, 34, 35, 39],\n",
            "        [43,  4, 16, 36, 32],\n",
            "        [42,  2, 39, 36, 15],\n",
            "        [16,  2,  8, 10, 33],\n",
            "        [14, 27, 43, 28,  0],\n",
            "        [21, 34,  2,  7, 22]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I49qjiqHB9oa"
      },
      "source": [
        "## Convolutional Neural Networks (60 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JePbG5pSt1xv"
      },
      "source": [
        "Implement a convolutional neural network for image classification on CIFAR-10 dataset.\n",
        "\n",
        "CIFAR-10 is an image dataset of 10 categories. Each image has a size of 32x32 pixels. The following code will download the dataset, and split it into `train` and `test`. For this question, we use the default validation split generated by Skorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQxOUQ29BuMB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "97913248d3b14d29b10bd8220325b9a0",
            "cfebcf125d80407996419345d1e38821",
            "0928af7f37594add8a04730faf0fddf2",
            "b336363bc36f451e9731bfc2befd7b3e",
            "4c31d2f799974c629f490eefbe33aac6",
            "13055ec7d9ad4be79a30edaa50cb7f08",
            "2e5f76add7844e299c926c1a817a6654",
            "bc3d1353159543149d5efcd467eb1f5c",
            "245793c77e34494b9e516f019d010be7",
            "3f176c4693254a8ea539796407c67a94",
            "abfa82ba68854fe686fb39c696e43e5b"
          ]
        },
        "outputId": "afbde10a-1079-4c0d-cc63-1caa588bd65c"
      },
      "source": [
        "train = torchvision.datasets.CIFAR10(\"./data\", train=True, download=True)\n",
        "test = torchvision.datasets.CIFAR10(\"./data\", train=False, download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97913248d3b14d29b10bd8220325b9a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53h9spYMo0Mq",
        "outputId": "5562e363-6ae6-4cfb-fe90-8f523e583477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieBpiwMwi6wD"
      },
      "source": [
        "The following code visualizes some samples in the dataset. You may use it to debug your model if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU5HrxybupyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f381e0f-5475-4a71-9a21-79a711f3b9a1"
      },
      "source": [
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# convert data to a normalized torch.FloatTensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.CIFAR10('data', train=True,\n",
        "                              download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10('data', train=False,\n",
        "                             download=True, transform=transform)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "    sampler=valid_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "    num_workers=num_workers)\n",
        "\n",
        "# specify the image classes\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "# Visualize the data set in order to do feature extraction\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "    \n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))# display 20 images\n",
        "for idx in np.arange(20):\n",
        "  ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "imshow(images[idx])\n",
        "ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'ship')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x288 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwzKmdcuCv1D"
      },
      "source": [
        "### 1) Basic CNN implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbEYo5WgjTtm"
      },
      "source": [
        "Consider a basic CNN model\n",
        "\n",
        "- It has 3 convolutional layers, followed by a linear layer.\n",
        "- Each convolutional layer has a kernel size of 3, a padding of 1.\n",
        "- ReLU activation is applied on every hidden layer.\n",
        "\n",
        "Please implement this model in the following section. You will need to tune the hyperparameters and fill the results in the table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZKyE2GUfL-Z"
      },
      "source": [
        "#### a) Implement convolutional layers (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P_aYytExtq9"
      },
      "source": [
        "Implement the initialization function and the forward function of the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDmCKUD1LBFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61ee119-984e-4091-c952-e8ba729c36db"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the CNN architecture\n",
        "class Net(nn.Module):  \n",
        "  \n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)  \n",
        "    \n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 16 * 5 * 5)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "# create a complete CNN\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "  model.cuda()\n",
        "\n",
        "import torch.optim as optim # specify loss function\n",
        "criterion = nn.CrossEntropyLoss() # specify optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# number of epochs to train the model\n",
        "n_epochs = 30\n",
        "\n",
        "#List to store loss to visualize\n",
        "train_losslist = []\n",
        "valid_loss_min = np.Inf \n",
        "\n",
        "# track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "\n",
        "# train the model #\n",
        "\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "# validate the model #\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)    \n",
        "    train_losslist.append(train_loss)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "Epoch: 1 \tTraining Loss: 1.738143 \tValidation Loss: 0.387248\n",
            "Validation loss decreased (inf --> 0.387248).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 1.400517 \tValidation Loss: 0.322652\n",
            "Validation loss decreased (0.387248 --> 0.322652).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 1.227258 \tValidation Loss: 0.293304\n",
            "Validation loss decreased (0.322652 --> 0.293304).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 1.135676 \tValidation Loss: 0.285008\n",
            "Validation loss decreased (0.293304 --> 0.285008).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 1.074916 \tValidation Loss: 0.265907\n",
            "Validation loss decreased (0.285008 --> 0.265907).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 1.023684 \tValidation Loss: 0.269777\n",
            "Epoch: 7 \tTraining Loss: 0.979784 \tValidation Loss: 0.254347\n",
            "Validation loss decreased (0.265907 --> 0.254347).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.938698 \tValidation Loss: 0.247414\n",
            "Validation loss decreased (0.254347 --> 0.247414).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.903711 \tValidation Loss: 0.239753\n",
            "Validation loss decreased (0.247414 --> 0.239753).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.871100 \tValidation Loss: 0.231183\n",
            "Validation loss decreased (0.239753 --> 0.231183).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.838780 \tValidation Loss: 0.230510\n",
            "Validation loss decreased (0.231183 --> 0.230510).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.810814 \tValidation Loss: 0.235497\n",
            "Epoch: 13 \tTraining Loss: 0.785926 \tValidation Loss: 0.241228\n",
            "Epoch: 14 \tTraining Loss: 0.759810 \tValidation Loss: 0.224624\n",
            "Validation loss decreased (0.230510 --> 0.224624).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.735695 \tValidation Loss: 0.227169\n",
            "Epoch: 16 \tTraining Loss: 0.714438 \tValidation Loss: 0.225168\n",
            "Epoch: 17 \tTraining Loss: 0.689250 \tValidation Loss: 0.224775\n",
            "Epoch: 18 \tTraining Loss: 0.668063 \tValidation Loss: 0.225682\n",
            "Epoch: 19 \tTraining Loss: 0.648201 \tValidation Loss: 0.234840\n",
            "Epoch: 20 \tTraining Loss: 0.628677 \tValidation Loss: 0.231738\n",
            "Epoch: 21 \tTraining Loss: 0.608078 \tValidation Loss: 0.231805\n",
            "Epoch: 22 \tTraining Loss: 0.589160 \tValidation Loss: 0.230312\n",
            "Epoch: 23 \tTraining Loss: 0.572246 \tValidation Loss: 0.235924\n",
            "Epoch: 24 \tTraining Loss: 0.551290 \tValidation Loss: 0.242527\n",
            "Epoch: 25 \tTraining Loss: 0.532331 \tValidation Loss: 0.245692\n",
            "Epoch: 26 \tTraining Loss: 0.517078 \tValidation Loss: 0.247922\n",
            "Epoch: 27 \tTraining Loss: 0.500035 \tValidation Loss: 0.256158\n",
            "Epoch: 28 \tTraining Loss: 0.485238 \tValidation Loss: 0.270992\n",
            "Epoch: 29 \tTraining Loss: 0.464702 \tValidation Loss: 0.268062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "plt.plot(n_epochs, train_losslist)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Performance of Model \")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "kkRi2g5IJvSK",
        "outputId": "4d879474-b30c-467a-afe4-14ed0c3c206d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7ce7680ff62a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losslist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performance of Model \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2761\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m   2763\u001b[0m         is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (29,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_YaASPpgRiL"
      },
      "source": [
        "#### b) Tune hyperparameters (20 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygMcDdpy6XWP"
      },
      "source": [
        "Train the CNN model on CIFAR-10 dataset. Tune the number of channels, optimizer, learning rate and the number of epochs for best validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PHxOHYnr8TU"
      },
      "source": [
        "# implement hyperparameters here\n",
        "model = skorch.NeuralNetClassifier(CNN, criterion=torch.nn.CrossEntropyLoss,\n",
        "                                   device=\"cuda\")\n",
        "class CNN(nn.Module):\n",
        "   \n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# implement input normalization & type cast here\n",
        "model.fit(train.data, train.targets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()# specify optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "sn9YWMz0xmYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-EHKzozkRbD"
      },
      "source": [
        "Write down **validation accuracy** of your model under different hyperparameter settings. Note the validation set is automatically split by Skorch during `model.fit()`.\n",
        "\n",
        "**Hint:** You may need more epochs for SGD than Adam.\n",
        "\n",
        "| #channel for each layer \\ optimizer | SGD   | Adam  |\n",
        "|-------------------------------------|-------|-------|\n",
        "| (128, 128, 128)                     |       |       |\n",
        "| (256, 256, 256)                     |       |       |\n",
        "| (512, 512, 512)                     |       |       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### c) Use larger CNN model (20 points)\n",
        "\n",
        "Add more Convolution/BatchNorm/Pooling/DropOut/Linear layers to improve the accuracy. Higher accuracy will get higher grade."
      ],
      "metadata": {
        "id": "fdQwD7doVo6j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjMkt8kVyxRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gadpfL_oywy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-h-x4VlywZ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}